{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np`\n",
    "import matplotlib.pyplot as pplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Dense in module tensorflow.python.keras.layers.core:\n",
      "\n",
      "class Dense(tensorflow.python.keras.engine.base_layer.Layer)\n",
      " |  Just your regular densely-connected NN layer.\n",
      " |  \n",
      " |  `Dense` implements the operation:\n",
      " |  `output = activation(dot(input, kernel) + bias)`\n",
      " |  where `activation` is the element-wise activation function\n",
      " |  passed as the `activation` argument, `kernel` is a weights matrix\n",
      " |  created by the layer, and `bias` is a bias vector created by the layer\n",
      " |  (only applicable if `use_bias` is `True`).\n",
      " |  \n",
      " |  Note: If the input to the layer has a rank greater than 2, then `Dense`\n",
      " |  computes the dot product between the `inputs` and the `kernel` along the\n",
      " |  last axis of the `inputs` and axis 1 of the `kernel` (using `tf.tensordot`).\n",
      " |  For example, if input has dimensions `(batch_size, d0, d1)`,\n",
      " |  then we create a `kernel` with shape `(d1, units)`, and the `kernel` operates\n",
      " |  along axis 2 of the `input`, on every sub-tensor of shape `(1, 1, d1)`\n",
      " |  (there are `batch_size * d0` such sub-tensors).\n",
      " |  The output in this case will have shape `(batch_size, d0, units)`.\n",
      " |  \n",
      " |  Besides, layer attributes cannot be modified after the layer has been called\n",
      " |  once (except the `trainable` attribute).\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  ```python\n",
      " |  # as first layer in a sequential model:\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32, input_shape=(16,)))\n",
      " |  # now the model will take as input arrays of shape (*, 16)\n",
      " |  # and output arrays of shape (*, 32)\n",
      " |  \n",
      " |  # after the first layer, you don't need to specify\n",
      " |  # the size of the input anymore:\n",
      " |  model.add(Dense(32))\n",
      " |  ```\n",
      " |  \n",
      " |  Arguments:\n",
      " |    units: Positive integer, dimensionality of the output space.\n",
      " |    activation: Activation function to use.\n",
      " |      If you don't specify anything, no activation is applied\n",
      " |      (ie. \"linear\" activation: `a(x) = x`).\n",
      " |    use_bias: Boolean, whether the layer uses a bias vector.\n",
      " |    kernel_initializer: Initializer for the `kernel` weights matrix.\n",
      " |    bias_initializer: Initializer for the bias vector.\n",
      " |    kernel_regularizer: Regularizer function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_regularizer: Regularizer function applied to the bias vector.\n",
      " |    activity_regularizer: Regularizer function applied to\n",
      " |      the output of the layer (its \"activation\")..\n",
      " |    kernel_constraint: Constraint function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_constraint: Constraint function applied to the bias vector.\n",
      " |  \n",
      " |  Input shape:\n",
      " |    N-D tensor with shape: `(batch_size, ..., input_dim)`.\n",
      " |    The most common situation would be\n",
      " |    a 2D input with shape `(batch_size, input_dim)`.\n",
      " |  \n",
      " |  Output shape:\n",
      " |    N-D tensor with shape: `(batch_size, ..., units)`.\n",
      " |    For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
      " |    the output would have shape `(batch_size, units)`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dense\n",
      " |      tensorflow.python.keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      tensorflow.python.keras.utils.version_utils.LayerVersionSelector\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the variables of the layer (optional, for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  call(self, inputs)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Input tensor, or list/tuple of input tensors.\n",
      " |          **kwargs: Additional keyword arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      If the layer has not been built, this method will call `build` on the\n",
      " |      layer. This assumes that the layer will later be used with inputs that\n",
      " |      match the input shape provided here.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        *args: Positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(inputs, self):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)), inputs=True)\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Activity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(x.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      The `get_losses_for` method allows to retrieve the losses relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        inputs: Ignored when executing eagerly. If anything other than None is\n",
      " |          passed, it signals the losses are conditional on some of the layer's\n",
      " |          inputs, and thus they should only be run where these inputs are\n",
      " |          available. This is the case for activity regularization losses, for\n",
      " |          instance. If `None` is passed, the losses are assumed\n",
      " |          to be unconditional, and will apply across all dataflows of the layer\n",
      " |          (e.g. weight regularization losses).\n",
      " |  \n",
      " |  add_metric(self, value, aggregation=None, name=None)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        aggregation: Sample-wise metric reduction function. If `aggregation=None`,\n",
      " |          it indicates that the metric tensor provided has been aggregated\n",
      " |          already. eg, `bin_acc = BinaryAccuracy(name='acc')` followed by\n",
      " |          `model.add_metric(bin_acc(y_true, y_pred))`. If aggregation='mean', the\n",
      " |          given metric tensor will be sample-wise reduced using `mean` function.\n",
      " |          eg, `model.add_metric(tf.reduce_sum(outputs), name='output_mean',\n",
      " |          aggregation='mean')`.\n",
      " |        name: String metric name.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `aggregation` is anything other than None or `mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(inputs)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      `inputs` is now automatically inferred\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      The `get_updates_for` method allows to retrieve the updates relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Arguments:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.add_weight` method instead.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        partitioner: Partitioner to be passed to the `Trackable` API.\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The created variable. Usually either a `Variable` or `ResourceVariable`\n",
      " |        instance. If `partitioner` is not `None`, a `PartitionedVariable`\n",
      " |        instance is returned.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called with partitioned variable regularization and\n",
      " |          eager execution is enabled.\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.__call__` method instead.\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      returns both trainable and non-trainable weight values associated with this\n",
      " |      layer as a list of Numpy arrays, which can in turn be used to load state\n",
      " |      into similarly parameterized layers.\n",
      " |      \n",
      " |      For example, a Dense layer returns a list of two values-- per-output\n",
      " |      weights and the bias value. These can be used to set the weights of another\n",
      " |      Dense layer:\n",
      " |      \n",
      " |      >>> a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b.set_weights(a.get_weights())\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      sets the weight values from numpy arrays. The weight values should be\n",
      " |      passed in the order they are created by the layer. Note that the layer's\n",
      " |      weights must be instantiated before calling this function by calling\n",
      " |      the layer.\n",
      " |      \n",
      " |      For example, a Dense layer returns a list of two values-- per-output\n",
      " |      weights and the bias value. These can be used to set the weights of another\n",
      " |      Dense layer:\n",
      " |      \n",
      " |      >>> a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b.set_weights(a.get_weights())\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Arguments:\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  dtype\n",
      " |      Dtype used by the weights of the layer, set in the constructor.\n",
      " |  \n",
      " |  dynamic\n",
      " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  input_spec\n",
      " |      `InputSpec` instance(s) describing the input format for this layer.\n",
      " |      \n",
      " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
      " |      the layer to run input compatibility checks when it is called.\n",
      " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
      " |      of rank 4. As such, you can set, in `__init__()`:\n",
      " |      \n",
      " |      ```python\n",
      " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
      " |      ```\n",
      " |      \n",
      " |      Now, if you try to call the layer on an input that isn't rank 4\n",
      " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
      " |      error:\n",
      " |      \n",
      " |      ```\n",
      " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
      " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
      " |      ```\n",
      " |      \n",
      " |      Input checks that can be specified via `input_spec` include:\n",
      " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
      " |      - Shape\n",
      " |      - Rank (ndim)\n",
      " |      - Dtype\n",
      " |      \n",
      " |      For more information, see `tf.keras.layers.InputSpec`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
      " |  \n",
      " |  losses\n",
      " |      Losses which are associated with this `Layer`.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |      List of `tf.keras.metrics.Metric` instances tracked by the layer.\n",
      " |  \n",
      " |  name\n",
      " |      Name of the layer (string), set in the constructor.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Non-trainable weights are *not* updated during training. They are expected\n",
      " |      to be updated manually in `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of non-trainable variables.\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Trainable weights are updated via gradient descent during training.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of trainable variables.\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      >>> class MyModule(tf.Module):\n",
      " |      ...   @tf.Module.with_name_scope\n",
      " |      ...   def __call__(self, x):\n",
      " |      ...     if not hasattr(self, 'w'):\n",
      " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
      " |      ...     return tf.matmul(x, self.w)\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      >>> mod = MyModule()\n",
      " |      >>> mod(tf.ones([1, 2]))\n",
      " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
      " |      >>> mod.w\n",
      " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
      " |      numpy=..., dtype=float32)>\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      >>> a = tf.Module()\n",
      " |      >>> b = tf.Module()\n",
      " |      >>> c = tf.Module()\n",
      " |      >>> a.b = b\n",
      " |      >>> b.c = c\n",
      " |      >>> list(a.submodules) == [b, c]\n",
      " |      True\n",
      " |      >>> list(b.submodules) == [c]\n",
      " |      True\n",
      " |      >>> list(c.submodules) == []\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from tensorflow.python.keras.utils.version_utils.LayerVersionSelector:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.layers.Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = tf.keras.layers.Input(shape = (2,))\n",
    "d1 = tf.keras.layers.Dense(50,'relu')(i1)\n",
    "d2= tf.keras.layers.Dense(2)(d1)\n",
    "m = tf.keras.Model(inputs = i1, outputs = d2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgrid = 20\n",
    "ygrid = 20\n",
    "minmax = 1\n",
    "xx,yy = np.meshgrid(np.linspace(-minmax,minmax,xgrid),np.linspace(-minmax,minmax,ygrid))\n",
    "p = np.vstack([xx.flatten(),yy.flatten()]).T\n",
    "\n",
    "\n",
    "napp = 20\n",
    "all_app = np.zeros([xgrid,ygrid,2, napp]);\n",
    "v = p\n",
    "all_app[:,:,:,0] = v.reshape(xgrid, ygrid,2)\n",
    "for k in range(0,napp-1):\n",
    "    pplot.pause(0.1)\n",
    "    v = m(v)\n",
    "    all_app[:,:,:,k+1] = v.numpy().reshape(xgrid, ygrid,2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab4b646da3e4bc2b4e75770a771de54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='kk', max=20), Output()), _dom_classes=('widget-interact'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact, interactive\n",
    "\n",
    "@interact(kk= IntSlider(min = 0 , max = napp))\n",
    "def aa(kk):\n",
    "    pplot.figure(figsize = (10,10))\n",
    "    pplot.pcolor(xx, yy, np.sum(np.abs(all_app[...,:,kk]),-1))\n",
    "    pplot.quiver(xx.flatten(), yy.flatten(), all_app[...,0,kk], all_app[...,1,kk])\n",
    "    pplot.colorbar()\n",
    "    pplot.title(kk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f22aa25fa90>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pplot.pcolor(np.max(np.max(np.abs(all_app),3),-1));pplot.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
